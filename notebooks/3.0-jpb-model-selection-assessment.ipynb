{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import (cross_val_score, \n",
    "                                     RandomizedSearchCV, \n",
    "                                     StratifiedKFold)\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                             classification_report,\n",
    "                             f1_score)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "PROJ_ROOT = os.pardir\n",
    "data_dir = join(PROJ_ROOT, 'data')\n",
    "\n",
    "src_dir = join(PROJ_ROOT, 'notebooks', 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from src.preprocessing import PreprocessorLinear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_data_train = join(PROJ_ROOT, 'data', 'bank_train_clean.csv')\n",
    "bank_data_val = join(PROJ_ROOT, 'data', 'bank_val_clean.csv')\n",
    "\n",
    "bank_train = pd.read_csv(bank_data_train)\n",
    "bank_val = pd.read_csv(bank_data_val)\n",
    "\n",
    "# get our target\n",
    "cust_response_train = bank_train['y']\n",
    "cust_response_val = bank_val['y']\n",
    "\n",
    "# get our features\n",
    "bank_feats_train = bank_train.drop('y', axis=1)\n",
    "bank_feats_val = bank_val.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize & dummy features for linear model\n",
    "preprocess = PreprocessorLinear()\n",
    "bank_feats_train = preprocess.fit_transform(bank_feats_train)\n",
    "bank_feats_val = preprocess.transform(bank_feats_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score --- 0.453818117168555\n",
      "Best parameters found:\n",
      "---{'penalty': 'l1', 'C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "lr = LogisticRegression(class_weight='balanced',\n",
    "                        random_state=12,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "param_distribution = {'penalty': ['l1', 'l2'],\n",
    "                      'C': [0.01, 0.05, 0.1, 0.5, 1.0,]}\n",
    "\n",
    "lr_rnd_search = RandomizedSearchCV(lr,\n",
    "                                   param_distribution,\n",
    "                                   n_iter=5,\n",
    "                                   scoring='f1',\n",
    "                                   n_jobs=-1,\n",
    "                                   cv=skf,\n",
    "                                   random_state=6)\n",
    "\n",
    "lr_rnd_search.fit(bank_feats_train, cust_response_train)\n",
    "score = lr_rnd_search.best_score_\n",
    "best_params = lr_rnd_search.best_params_\n",
    "\n",
    "print(f'Best f1 score --- {score}')\n",
    "print(f'Best parameters found:\\n---{best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best = lr_rnd_search.best_estimator_\n",
    "lr_best.fit(bank_feats_train, cust_response_train)\n",
    "\n",
    "cust_predictions = lr_best.predict(bank_feats_val)\n",
    "cust_true = cust_response_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Pred_Neg  Pred_Pos\n",
      "True_Neg      6175      1058\n",
      "True_Pos       342       573\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90      7233\n",
      "           1       0.35      0.63      0.45       915\n",
      "\n",
      "    accuracy                           0.83      8148\n",
      "   macro avg       0.65      0.74      0.67      8148\n",
      "weighted avg       0.88      0.83      0.85      8148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(confusion_matrix(\n",
    "    cust_true, cust_predictions), columns=['Pred_Neg', 'Pred_Pos'], index=['True_Neg', 'True_Pos'])\n",
    ")\n",
    "print(f'\\n{classification_report(cust_true, cust_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score --- 0.5052416289977009\n",
      "Best parameters found:\n",
      "---{'min_samples_leaf': 10, 'max_features': 0.75, 'max_depth': 15}\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "rf = RandomForestClassifier(n_estimators=100, \n",
    "                            class_weight='balanced', \n",
    "                            random_state=6, n_jobs=-1)\n",
    "\n",
    "param_distribution = {'max_features': [0.8, 0.75, 0.7, 0.65],\n",
    "                      'max_depth': [5, 10, 12, 15, 18, 20],\n",
    "                      'min_samples_leaf': [7, 10, 12, 15, 20]}\n",
    "\n",
    "rf_rnd_search = RandomizedSearchCV(rf,\n",
    "                                   param_distribution,\n",
    "                                   n_iter=10,\n",
    "                                   scoring='f1',\n",
    "                                   n_jobs=-1,\n",
    "                                   cv=skf,\n",
    "                                   random_state=9)\n",
    "\n",
    "rf_rnd_search.fit(bank_feats_train, cust_response_train)\n",
    "rf_score = rf_rnd_search.best_score_\n",
    "rf_best_params = rf_rnd_search.best_params_\n",
    "\n",
    "print(f'Best f1 score --- {rf_score}')\n",
    "print(f'Best parameters found:\\n---{rf_best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Pred_Neg  Pred_Pos\n",
      "True_Neg      6498       735\n",
      "True_Pos       382       533\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92      7233\n",
      "           1       0.42      0.58      0.49       915\n",
      "\n",
      "    accuracy                           0.86      8148\n",
      "   macro avg       0.68      0.74      0.70      8148\n",
      "weighted avg       0.89      0.86      0.87      8148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_best = rf_rnd_search.best_estimator_\n",
    "rf_best.fit(bank_feats_train, cust_response_train)\n",
    "\n",
    "rf_cust_predictions = rf_best.predict(bank_feats_val)\n",
    "\n",
    "print(pd.DataFrame(confusion_matrix(\n",
    "    cust_true, rf_cust_predictions), columns=['Pred_Neg', 'Pred_Pos'], index=['True_Neg', 'True_Pos'])\n",
    ")\n",
    "print(f'\\n{classification_report(cust_true, rf_cust_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #1\n",
      "Best f1 so far: 0.443141\n",
      "Best params: OrderedDict([('booster', 'dart'), ('colsample_bylevel', 0.4033701620801624), ('colsample_bytree', 0.9205557175962006), ('gamma', 0.0020222673119915627), ('learning_rate', 0.04210159788003544), ('max_delta_step', 8), ('max_depth', 37), ('min_child_weight', 0), ('n_estimators', 150), ('reg_alpha', 0.8371075327297838), ('reg_lambda', 150), ('scale_pos_weight', 10), ('subsample', 0.1625820296298843)])\n",
      "\n",
      "Model #2\n",
      "Best f1 so far: 0.443141\n",
      "Best params: OrderedDict([('booster', 'dart'), ('colsample_bylevel', 0.4033701620801624), ('colsample_bytree', 0.9205557175962006), ('gamma', 0.0020222673119915627), ('learning_rate', 0.04210159788003544), ('max_delta_step', 8), ('max_depth', 37), ('min_child_weight', 0), ('n_estimators', 150), ('reg_alpha', 0.8371075327297838), ('reg_lambda', 150), ('scale_pos_weight', 10), ('subsample', 0.1625820296298843)])\n",
      "\n",
      "Model #3\n",
      "Best f1 so far: 0.478303\n",
      "Best params: OrderedDict([('booster', 'gbtree'), ('colsample_bylevel', 0.6387056328729683), ('colsample_bytree', 0.5236418167742088), ('gamma', 3.51664980271989e-05), ('learning_rate', 0.27142043278980077), ('max_delta_step', 0), ('max_depth', 12), ('min_child_weight', 5), ('n_estimators', 125), ('reg_alpha', 0.23295143607417343), ('reg_lambda', 445), ('scale_pos_weight', 7), ('subsample', 0.5505594690000204)])\n",
      "\n",
      "Model #4\n",
      "Best f1 so far: 0.478303\n",
      "Best params: OrderedDict([('booster', 'gbtree'), ('colsample_bylevel', 0.6387056328729683), ('colsample_bytree', 0.5236418167742088), ('gamma', 3.51664980271989e-05), ('learning_rate', 0.27142043278980077), ('max_delta_step', 0), ('max_depth', 12), ('min_child_weight', 5), ('n_estimators', 125), ('reg_alpha', 0.23295143607417343), ('reg_lambda', 445), ('scale_pos_weight', 7), ('subsample', 0.5505594690000204)])\n",
      "\n",
      "Model #5\n",
      "Best f1 so far: 0.478303\n",
      "Best params: OrderedDict([('booster', 'gbtree'), ('colsample_bylevel', 0.6387056328729683), ('colsample_bytree', 0.5236418167742088), ('gamma', 3.51664980271989e-05), ('learning_rate', 0.27142043278980077), ('max_delta_step', 0), ('max_depth', 12), ('min_child_weight', 5), ('n_estimators', 125), ('reg_alpha', 0.23295143607417343), ('reg_lambda', 445), ('scale_pos_weight', 7), ('subsample', 0.5505594690000204)])\n",
      "\n",
      "Model #6\n",
      "Best f1 so far: 0.501031\n",
      "Best params: OrderedDict([('booster', 'dart'), ('colsample_bylevel', 0.2640177169703445), ('colsample_bytree', 0.7555154988531825), ('gamma', 0.04681395435243262), ('learning_rate', 0.49056467593489167), ('max_delta_step', 7), ('max_depth', 49), ('min_child_weight', 5), ('n_estimators', 125), ('reg_alpha', 0.00048817066428318977), ('reg_lambda', 382), ('scale_pos_weight', 4), ('subsample', 0.6718602766530064)])\n",
      "\n",
      "Model #7\n",
      "Best f1 so far: 0.50591\n",
      "Best params: OrderedDict([('booster', 'gbtree'), ('colsample_bylevel', 0.45542071920812965), ('colsample_bytree', 0.29482216508835213), ('gamma', 0.033805567611141965), ('learning_rate', 0.4904633141001969), ('max_delta_step', 4), ('max_depth', 44), ('min_child_weight', 3), ('n_estimators', 100), ('reg_alpha', 0.0030419479086777825), ('reg_lambda', 475), ('scale_pos_weight', 4), ('subsample', 0.9857542005357888)])\n",
      "\n",
      "Model #8\n",
      "Best f1 so far: 0.50591\n",
      "Best params: OrderedDict([('booster', 'gbtree'), ('colsample_bylevel', 0.45542071920812965), ('colsample_bytree', 0.29482216508835213), ('gamma', 0.033805567611141965), ('learning_rate', 0.4904633141001969), ('max_delta_step', 4), ('max_depth', 44), ('min_child_weight', 3), ('n_estimators', 100), ('reg_alpha', 0.0030419479086777825), ('reg_lambda', 475), ('scale_pos_weight', 4), ('subsample', 0.9857542005357888)])\n",
      "\n",
      "Model #9\n",
      "Best f1 so far: 0.50591\n",
      "Best params: OrderedDict([('booster', 'gbtree'), ('colsample_bylevel', 0.45542071920812965), ('colsample_bytree', 0.29482216508835213), ('gamma', 0.033805567611141965), ('learning_rate', 0.4904633141001969), ('max_delta_step', 4), ('max_depth', 44), ('min_child_weight', 3), ('n_estimators', 100), ('reg_alpha', 0.0030419479086777825), ('reg_lambda', 475), ('scale_pos_weight', 4), ('subsample', 0.9857542005357888)])\n",
      "\n",
      "[13:01:27] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, \n",
    "                            tree_method='approx',\n",
    "                            n_jobs=-1)\n",
    "\n",
    "xgb_bayes_search = BayesSearchCV(\n",
    "    estimator = xgb_clf,\n",
    "    search_spaces = {\n",
    "        'learning_rate': (0.001, 1.0, 'log-uniform'),\n",
    "        'min_child_weight': (0, 10),\n",
    "        'max_depth': (0, 50),\n",
    "        'booster': ('gbtree', 'dart'),\n",
    "        'max_delta_step': (0, 20),\n",
    "        'subsample': (0.01, 1.0, 'uniform'),\n",
    "        'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "        'colsample_bylevel': (0.01, 1.0, 'uniform'),\n",
    "        'reg_lambda': (1e-5, 1000, 'log-uniform'),\n",
    "        'reg_alpha': (1e-5, 1.0, 'log-uniform'),\n",
    "        'gamma': (1e-6, 0.5, 'log-uniform'),\n",
    "        'min_child_weight': (0, 5),\n",
    "        'n_estimators': (50, 75, 100, 125, 150),\n",
    "        'scale_pos_weight': (1, 4, 7, 10, 12, 15, 20)\n",
    "    },    \n",
    "    scoring = 'f1',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=5,\n",
    "        shuffle=True,\n",
    "        random_state=24\n",
    "    ),\n",
    "    n_jobs=-1,\n",
    "    n_iter=9,   \n",
    "    verbose=0,\n",
    "    refit=True,\n",
    "    random_state=24 \n",
    ")\n",
    "\n",
    "def model_hyperparameters(optim_result):\n",
    "    \"\"\"Status callback durring hyperparameter search\"\"\"\n",
    "    \n",
    "    models = pd.DataFrame(xgb_bayes_search.cv_results_)    \n",
    "    \n",
    "    # current and best parameters    \n",
    "    best_params = pd.Series(xgb_bayes_search.best_params_)\n",
    "    print(f'Model #{len(models)}\\nBest f1 so far: {np.round(xgb_bayes_search.best_score_, 6)}'\n",
    "          f'\\nBest params: {xgb_bayes_search.best_params_}\\n')\n",
    "\n",
    "results = xgb_bayes_search.fit(\n",
    "    bank_feats_train, cust_response_train, callback=model_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:01:29] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "          Pred_Neg  Pred_Pos\n",
      "True_Neg      6597       636\n",
      "True_Pos       405       510\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93      7233\n",
      "           1       0.45      0.56      0.49       915\n",
      "\n",
      "    accuracy                           0.87      8148\n",
      "   macro avg       0.69      0.73      0.71      8148\n",
      "weighted avg       0.89      0.87      0.88      8148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_best = xgb_bayes_search.best_estimator_\n",
    "xgb_best.fit(bank_feats_train, cust_response_train)\n",
    "\n",
    "xgb_cust_predictions = xgb_best.predict(bank_feats_val)\n",
    "\n",
    "print(pd.DataFrame(confusion_matrix(\n",
    "    cust_response_val, xgb_cust_predictions), columns=['Pred_Neg', 'Pred_Pos'], index=['True_Neg', 'True_Pos'])\n",
    ")\n",
    "print(f'\\n{classification_report(cust_response_val, xgb_cust_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
